{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3a30e5",
   "metadata": {},
   "source": [
    "# Baseline Solution\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae68e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600387d6",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a7e80-e745-4c1f-9c0d-b4e40b1ba931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsururu\n",
      "  Downloading tsururu-1.1.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting holidays<0.41,>=0.40 (from tsururu)\n",
      "  Downloading holidays-0.40-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.3 (from tsururu)\n",
      "  Downloading numpy-1.26.4.tar.gz (15.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l/"
     ]
    }
   ],
   "source": [
    "pip install tsururu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11beae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN amount: 579\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"frames_errors.csv\", header=None)\n",
    "df.columns = [\n",
    "    \"block_id\",\n",
    "    \"frame_idx\",\n",
    "    \"E_mu_Z\",\n",
    "    \"E_mu_phys_est\",\n",
    "    \"E_mu_X\",\n",
    "    \"E_nu1_X\",\n",
    "    \"E_nu2_X\",\n",
    "    \"E_nu1_Z\",\n",
    "    \"E_nu2_Z\",\n",
    "    \"N_mu_X\",\n",
    "    \"M_mu_XX\",\n",
    "    \"M_mu_XZ\",\n",
    "    \"M_mu_X\",\n",
    "    \"N_mu_Z\",\n",
    "    \"M_mu_ZZ\",\n",
    "    \"M_mu_Z\",\n",
    "    \"N_nu1_X\",\n",
    "    \"M_nu1_XX\",\n",
    "    \"M_nu1_XZ\",\n",
    "    \"M_nu1_X\",\n",
    "    \"N_nu1_Z\",\n",
    "    \"M_nu1_ZZ\",\n",
    "    \"M_nu1_Z\",\n",
    "    \"N_nu2_X\",\n",
    "    \"M_nu2_XX\",\n",
    "    \"M_nu2_XZ\",\n",
    "    \"M_nu2_X\",\n",
    "    \"N_nu2_Z\",\n",
    "    \"M_nu2_ZZ\",\n",
    "    \"M_nu2_Z\",\n",
    "    \"nTot\",\n",
    "    \"bayesImVoltage\",\n",
    "    \"opticalPower\",\n",
    "    \"polarizerVoltages[0]\",\n",
    "    \"polarizerVoltages[1]\",\n",
    "    \"polarizerVoltages[2]\",\n",
    "    \"polarizerVoltages[3]\",\n",
    "    \"temp_1\",\n",
    "    \"biasVoltage_1\",\n",
    "    \"temp_2\",\n",
    "    \"biasVoltage_2\",\n",
    "    \"synErr\",\n",
    "    \"N_EC_rounds\",\n",
    "    \"maintenance_flag\",\n",
    "    \"estimator_name\",\n",
    "    \"f_EC\",\n",
    "    \"E_mu_Z_est\",\n",
    "    \"R\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "]\n",
    "\n",
    "df_base = df.drop(\n",
    "    [\n",
    "        \"E_mu_phys_est\",\n",
    "        \"f_EC\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(f\"NaN amount: {df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebaf76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f701bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame count/Series count\n",
      "date\n",
      "399    569\n",
      "400    251\n",
      "398      2\n",
      "390      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(\n",
    "    columns={\n",
    "        \"block_id\": \"id\",\n",
    "        \"E_mu_Z\": \"value\",\n",
    "        \"frame_idx\": \"date\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Looking at the length of the time series by the number of frames\n",
    "timestamp_counts = df.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Frame count/Series count\")\n",
    "print(timestamp_counts.value_counts())\n",
    "\n",
    "df_for_ts = df[[\"id\", \"value\", \"date\"]].dropna(subset=[\"value\"], how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91ebc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame count/Series count\n",
      "date\n",
      "400    815\n",
      "399      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_for_ts = df_for_ts.set_index([\"id\", \"date\"]).unstack().ffill().stack().reset_index()\n",
    "timestamp_counts = df_for_ts.groupby(\"id\")[\"date\"].nunique()\n",
    "print(\"Frame count/Series count\")\n",
    "print(timestamp_counts.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d282ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leftout segments: 815\n"
     ]
    }
   ],
   "source": [
    "df_for_ts = df_for_ts.groupby(\"id\").filter(lambda x: len(x) == 400)\n",
    "print(\"Leftout segments:\", df_for_ts[\"id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b1187",
   "metadata": {},
   "source": [
    "# DLinear\n",
    "\n",
    "`DLinear` is a simple and fast model that extracts the trend using AveragePooling, then applies nn.Linear to both the trend and residual components, and finally combines everything back together. You can learn more about the model in the paper:  https://arxiv.org/abs/2205.13504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49af1e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tsururu'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtsururu\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline, TSDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtsururu\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DLTrainer\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtsururu\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HoldOutValidator\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tsururu'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "\n",
    "c_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(c_handler)\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import Module\n",
    "\n",
    "from tsururu.dataset import Pipeline, TSDataset\n",
    "from tsururu.model_training.trainer import DLTrainer\n",
    "from tsururu.model_training.validator import HoldOutValidator\n",
    "from tsururu.strategies import RecursiveStrategy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd932e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(Module):\n",
    "    \"\"\"Moving average block for extracting the trend of a time series.\n",
    "\n",
    "    Args:\n",
    "        kernel_size: window size of the convolution (kernel).\n",
    "        stride: step size of the moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> \"torch.Tensor\":\n",
    "        \"\"\"Forward pass for computing the moving average.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor after applying the moving average.\n",
    "        \"\"\"\n",
    "        # add padding (repeat boundary values) on both sides of the time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "\n",
    "        # apply moving average along the time axis\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(Module):\n",
    "    \"\"\"Time series decomposition block.\n",
    "\n",
    "    Args:\n",
    "        kernel_size: window size for the moving average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n",
    "        \"\"\"Forward pass for decomposing the series into trend and residual.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of tensors (residual, trend).\n",
    "        \"\"\"\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class DLinear_NN(Module):\n",
    "    def __init__(self, features_groups, pred_len, seq_len, moving_avg=25, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Protection against type \"wrappers\"\n",
    "        def _to_int(x):\n",
    "            if isinstance(x, int):\n",
    "                return x\n",
    "            if isinstance(x, dict) and \"value\" in x:\n",
    "                return int(x[\"value\"])\n",
    "            try:\n",
    "                return int(x)\n",
    "            except Exception:\n",
    "                raise TypeError(f\"Expected int-like, got {type(x)}: {x}\")\n",
    "\n",
    "        # If named arguments are also passed, we'll take them so they don't interfere\n",
    "        seq_len = _to_int(kwargs.pop(\"seq_len\", seq_len))\n",
    "        pred_len = _to_int(kwargs.pop(\"pred_len\", pred_len))\n",
    "        moving_avg = int(kwargs.pop(\"moving_avg\", moving_avg))\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.decompsition = series_decomp(moving_avg)\n",
    "        self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "        self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "        self.Linear_Seasonal.weight = nn.Parameter(\n",
    "            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len])\n",
    "        )\n",
    "        self.Linear_Trend.weight = nn.Parameter(\n",
    "            (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len])\n",
    "        )\n",
    "\n",
    "    def forward(self, x: \"torch.Tensor\") -> \"torch.Tensor\":\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, seq_len, num_features).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, pred_len, num_features).\n",
    "        \"\"\"\n",
    "        # Decompose the time series into trend and residual (seasonality)\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "\n",
    "        # Transpose tensors to the format (batch_size, num_features, seq_len)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0, 2, 1), trend_init.permute(\n",
    "            0, 2, 1\n",
    "        )\n",
    "\n",
    "        # Apply linear layers to trend and residuals\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        # Sum the results of the linear layers\n",
    "        x = seasonal_output + trend_output\n",
    "\n",
    "        # Transpose back to the format (batch_size, seq_len, num_features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x[:, -self.pred_len :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7403fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will predict 8 values ahead using a window of 160\n",
    "\n",
    "HORIZON = 8\n",
    "HISTORY = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "val_df = []\n",
    "test_df = []\n",
    "test_targets = []\n",
    "for current_id in df_for_ts[\"id\"].unique():\n",
    "    current_df = df_for_ts[df_for_ts[\"id\"] == current_id]\n",
    "    train_df.append(current_df.iloc[: -2 * HORIZON])\n",
    "    val_df.append(current_df.iloc[-2 * HORIZON - HISTORY : -HORIZON])\n",
    "    test_df.append(current_df.iloc[-HORIZON - HISTORY : -HORIZON])\n",
    "    test_targets.append(current_df.iloc[-HORIZON:])\n",
    "train_df = pd.concat(train_df)\n",
    "val_df = pd.concat(val_df)\n",
    "test_df = pd.concat(test_df)\n",
    "test_targets = pd.concat(test_targets)\n",
    "\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Validation set shape: {val_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Test target shape: {test_targets.shape}\")\n",
    "\n",
    "print(f\"Number of series in the training set: {train_df['id'].nunique()}\")\n",
    "print(f\"Number of series in the validation set: {val_df['id'].nunique()}\")\n",
    "print(f\"Number of series in the test set: {test_df['id'].nunique()}\")\n",
    "print(f\"Number of series in the test targets: {test_targets['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa379e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base date (first day)\n",
    "# This is required for correct operation of the tsururu library and does not affect the core task\n",
    "\n",
    "base_date = pd.to_datetime(\"2000-01-01\")\n",
    "\n",
    "\n",
    "def convert_dates(series):\n",
    "    return base_date + pd.to_timedelta(series.astype(int) - 1, unit=\"D\")\n",
    "\n",
    "\n",
    "# Apply to every DataFrame\n",
    "\n",
    "train_df[\"date\"] = convert_dates(train_df[\"date\"])\n",
    "val_df[\"date\"] = convert_dates(val_df[\"date\"])\n",
    "test_df[\"date\"] = convert_dates(test_df[\"date\"])\n",
    "test_targets[\"date\"] = convert_dates(test_targets[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e36220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "dataset_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"type\": \"continuous\",\n",
    "    },\n",
    "    \"date\": {\n",
    "        \"columns\": [\"date\"],\n",
    "        \"type\": \"datetime\",\n",
    "    },\n",
    "    \"id\": {\n",
    "        \"columns\": [\"id\"],\n",
    "        \"type\": \"categorical\",\n",
    "    },\n",
    "}\n",
    "\n",
    "train_dataset = TSDataset(\n",
    "    data=train_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=True,\n",
    ")\n",
    "val_dataset = TSDataset(\n",
    "    data=val_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")\n",
    "test_dataset = TSDataset(\n",
    "    data=test_df,\n",
    "    columns_params=dataset_params,\n",
    "    print_freq_period_info=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90dfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"target\": {\n",
    "        \"columns\": [\"value\"],\n",
    "        \"features\": {\n",
    "            \"DifferenceNormalizer\": {\n",
    "                \"regime\": \"delta\",\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"MissingValuesImputer\": {  # After DifferenceNormalizer, NaNs inevitably appear in the data (at the first value of each segment)\n",
    "                \"constant_value\": 0,  # Fill them with zeros\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "            },\n",
    "            \"StandardScalerTransformer\": {  # And align the series values before feeding them into the DL model\n",
    "                \"transform_target\": True,\n",
    "                \"transform_features\": True,\n",
    "                \"agg_by_id\": True,\n",
    "            },\n",
    "            \"LagTransformer\": {\"lags\": HISTORY},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60966ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = choose_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train setup\n",
    "\n",
    "pipeline = Pipeline.from_dict(pipeline_params, multivariate=False)\n",
    "\n",
    "validation = HoldOutValidator\n",
    "validation_params = {\"validation_data\": val_dataset}\n",
    "\n",
    "trainer_params = {\n",
    "    \"device\": DEVICE,\n",
    "    \"num_workers\": 4,\n",
    "    \"best_by_metric\": True,\n",
    "    \"save_to_dir\": False,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_epochs\": 5,\n",
    "    \"early_stopping_patience\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "trainer = DLTrainer(\n",
    "    model=DLinear_NN,\n",
    "    model_params={\"moving_avg\": 25},\n",
    "    validator=validation,\n",
    "    validation_params=validation_params,\n",
    "    **trainer_params,\n",
    ")\n",
    "\n",
    "\n",
    "strategy = RecursiveStrategy(\n",
    "    horizon=HORIZON,\n",
    "    model_horizon=4,\n",
    "    history=HISTORY,\n",
    "    pipeline=pipeline,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4030151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "fit_time, metrics = strategy.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a003af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "import pickle\n",
    "\n",
    "model_filename = \"dlinear_strategy.pkl\"\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(strategy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model to start predicting\n",
    "\n",
    "with open(model_filename, \"rb\") as f:\n",
    "    loaded_strategy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b704b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time, current_pred = loaded_strategy.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee464704",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pred = current_pred.sort_values([\"id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "ids = current_pred[\"id\"].unique().tolist()\n",
    "n_ids = len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e53869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to return 2000 data points\n",
    "TOTAL = 2000\n",
    "base = TOTAL // n_ids  # Base number of points per id\n",
    "rem = TOTAL % n_ids  # For the first rem ids, give 1 extra point\n",
    "\n",
    "if base == 0:\n",
    "    # Case when there are too many series (n_ids > 2000): take 1 point for the first 2000 ids\n",
    "    selected_ids = ids[:TOTAL]\n",
    "    compressed_values = []\n",
    "    for i in selected_ids:\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "        # Take, for example, the last value of the horizon\n",
    "        compressed_values.append(float(arr[-1]))\n",
    "else:\n",
    "    # Normal case (~815 series): base=2, rem=2000-2*815=370 => 370 series will get 3 points, the rest 2\n",
    "    compressed_values = []\n",
    "    for idx, i in enumerate(ids):\n",
    "        k = base + (1 if idx < rem else 0)  # Target points for this id\n",
    "        arr = current_pred.loc[current_pred[\"id\"] == i, \"value\"].to_numpy()\n",
    "\n",
    "        # Safety check: if horizon < k (shouldn't happen), just repeat the last values\n",
    "        if len(arr) < k:\n",
    "            arr = np.pad(arr, (0, k - len(arr)), mode=\"edge\")\n",
    "\n",
    "        # Split into k ~equal parts and average each\n",
    "        chunks = np.array_split(arr, k)\n",
    "        means = [float(np.mean(c)) for c in chunks]\n",
    "        compressed_values.extend(means)\n",
    "\n",
    "# Obtain exactly 2000 values in a fixed order\n",
    "target_df = pd.DataFrame({\"value\": compressed_values})\n",
    "assert len(target_df) == 2000, f\"Got {len(target_df)} instead of 2000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "alpha = 0.33\n",
    "f_ec = 1.15\n",
    "R_range = [\n",
    "    round(0.50 + 0.05 * x, 2) for x in range(9)\n",
    "]  # 0.50..0.90 to match the task requirements\n",
    "n = 32000\n",
    "d = 4800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ema(prev_ema, current_value, alpha):\n",
    "    if prev_ema is None:\n",
    "        return current_value\n",
    "    return alpha * current_value + (1 - alpha) * prev_ema\n",
    "\n",
    "\n",
    "def h(x):\n",
    "    if x > 0:\n",
    "        return -x * np.log2(x) - (1 - x) * np.log2(1 - x)\n",
    "    elif x == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Invalid x for binary entropy\")\n",
    "\n",
    "\n",
    "def select_code_rate(e_mu, f_ec, rates, frame_len, sp_count):\n",
    "    r_candidate = 1 - h(e_mu) * f_ec\n",
    "    R_res = 0.50\n",
    "    s_n = sp_count\n",
    "    p_n = 0\n",
    "    for R in rates:\n",
    "        p_n = int(\n",
    "            ceil((1 - R) * frame_len - (1 - r_candidate) * (frame_len - sp_count))\n",
    "        )\n",
    "        s_n = int(sp_count - p_n)\n",
    "        if p_n >= 0 and s_n >= 0:\n",
    "            R_res = R\n",
    "            return round(R_res, 2), s_n, p_n\n",
    "    return round(R_res, 2), s_n, p_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555058a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_series = (\n",
    "    pd.to_numeric(target_df.iloc[:, 0], errors=\"coerce\").dropna().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "prev_ema = None\n",
    "rows = []\n",
    "for E_mu_Z in E_series:\n",
    "    ema_value = calculate_ema(prev_ema, float(E_mu_Z), alpha)\n",
    "    prev_ema = ema_value\n",
    "    R, s_n, p_n = select_code_rate(ema_value, f_ec, R_range, n, d)\n",
    "    rows.append([f\"{E_mu_Z:.16f}\", R, s_n, p_n])  # 4 columns: E, R, s_n, p_n\n",
    "\n",
    "# Save the submission\n",
    "pd.DataFrame(rows).to_csv(\"submission.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bcf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
